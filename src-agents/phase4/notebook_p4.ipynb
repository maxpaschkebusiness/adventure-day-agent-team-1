{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 - Efficiency\n",
    "\n",
    "This lab is about making sure you use less tokens - because tokens is the \"currency\" for AI requests and we want to save money! \n",
    "The way we tackle it in this lab is by caching questions and answers - so you don't need to ask your LLM all the time. To make this a little more sophisticated we are generating vectors for the questions - and therefore cache the semantics of the questions, not just the string.\n",
    "\n",
    "Below you will find a sample how to do this.\n",
    "It's your job to build this caching into your api so you can reduce the number of tokens used. Are there other optimizations you could think of?\n",
    "\n",
    "\n",
    "\n",
    "If not already done run this in the top level folder:\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Azure OpenAI API Base Endpoint: https://cog-ukzrhgnqdd2wy.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "if load_dotenv():\n",
    "    print(\"Found Azure OpenAI API Base Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "else: \n",
    "    print(\"Azure OpenAI API Base Endpoint not found. Have you configured the .env file?\")\n",
    "    \n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "API_VERSION = os.getenv(\"OPENAI_API_VERSION\")\n",
    "RESOURCE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    ")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "model_name = os.getenv(\"AZURE_OPENAI_COMPLETION_MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the object model for receiving questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class QuestionType(str, Enum):\n",
    "    multiple_choice = \"multiple_choice\"\n",
    "    true_or_false = \"true_or_false\"\n",
    "    popular_choice = \"popular_choice\"\n",
    "    estimation = \"estimation\"\n",
    "\n",
    "class Ask(BaseModel):\n",
    "    question: str | None = None\n",
    "    type: QuestionType\n",
    "    correlationToken: str | None = None\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: str\n",
    "    correlationToken: str | None = None\n",
    "    promptTokensUsed: int | None = None\n",
    "    completionTokensUsed: int | None = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the number of tokens\n",
    "tiktoken is a library which allows you to get the number of tokens. This will allow you to check how much tokens you've been using.\n",
    "Ensure you pick the correct encoding for your model based on this list. https://github.com/openai/tiktoken/blob/c0ba74c238d18b4824c25f3c27fc8698055b9a76/tiktoken/model.py#L20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the string: 7\n"
     ]
    }
   ],
   "source": [
    "def get_num_tokens_from_string(string: str, encoding_name: str='p50k_base') -> int:\n",
    "    \"\"\"Returns the number of tokens in a text by a given encoding.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "number_of_tokens=get_num_tokens_from_string(\"Hello, Azure AI Adventure Day!\")\n",
    "print(f\"Number of tokens in the string: {number_of_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use AI Search for semantic caching\n",
    "The snippets below show you how we cache the semantic meaning of questions into Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use an embeddingsmodel to create embeddings\n",
    "def get_embedding(text, model=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")):\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex\n",
    "\n",
    ")\n",
    "\n",
    "credential = AzureKeyCredential(os.environ[\"AZURE_AI_SEARCH_KEY\"]) if len(os.environ[\"AZURE_AI_SEARCH_KEY\"]) > 0 else DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new index to store questions and answers - and the vector which represents the semantic of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " question-semantic-index created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index_name = \"question-semantic-index\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"], \n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# Create a search index with the fields and a vector field which we will fill with a vector based on the overview field\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"question\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"answer\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configure the semantic search configuration \n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"question-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"question\"),\n",
    "        keywords_fields=[SemanticField(field_name=\"answer\")],\n",
    "        content_fields=[SemanticField(field_name=\"question\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import some test data. As you see below the test data set contains 3 questions - all asking for the same with different words. So the semantics are the same, but it's not a word by word match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "questions = [{\n",
    "        \"question\": \"Which actor plays Tony Stark in the Marvel movies?\",\n",
    "        \"answer\": \"Robert Downey Jr.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In the Marvel Cinematic Universe, who is the actor that brings Tony Stark to life?\",\n",
    "        \"answer\": \"Robert Downey Jr.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who brings the character of Tony Stark to life in the Marvel Cinematic Universe?\",\n",
    "        \"answer\": \"Robert Downey Jr.\"\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes, we put those questions into the index. To do this, we create a vector for all questions which represent the meaning of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embedding for the question\n",
    "index = 1\n",
    "for question in questions:\n",
    "    question[\"id\"] = str(index)\n",
    "    question[\"vector\"] = get_embedding(question[\"question\"])\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put all those questions into our new index of Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3 questions into Azure AI Search index.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# create new searchclient using our new index for the questions\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"], \n",
    "    index_name=index_name,\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# upload question to vector store\n",
    "result = search_client.upload_documents(questions)\n",
    "print(f\"Successfully loaded {len(questions)} questions into Azure AI Search index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a try - ask the same question again - this time with yet another sentence but with the same semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is Tony Stark in the MCU?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we don't just ask the LLM - instead we generate an embedding and search for the vector in our new index - and get the top 5 questions and answers and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery\n",
    ")\n",
    "\n",
    "# create a vectorized query based on the question\n",
    "vector = VectorizedQuery(vector=get_embedding(question), k_nearest_neighbors=5, fields=\"vector\")\n",
    "\n",
    "# create search client to retrieve movies from the vector store\n",
    "found_questions = list(search_client.search(\n",
    "    search_text=None,\n",
    "    query_type=\"semantic\",\n",
    "    semantic_configuration_name=\"question-semantic-config\",\n",
    "    vector_queries=[vector],\n",
    "    select=[\"question\", \"answer\"],\n",
    "    top=5\n",
    "))\n",
    "\n",
    "# print the found documents and the field that were selected\n",
    "for result in found_questions:\n",
    "    print(\"Question: {}\".format(result[\"question\"]))\n",
    "    print(\"Answer: {}\".format(result[\"answer\"]))\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen, we can get the correct answer for a question that was never asked the same way before, if we manage to cache questions and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUR Mission: \n",
    "Adjust the function below and reuse it in the main.py file later to deploy to Azure and to update your service. \n",
    "Ensure the answers provided are correct and in the correct format.\n",
    "\n",
    "- for incomingn questions, create a vector embedding\n",
    "- check if the answer is in the cache before \n",
    "- if yes, \n",
    "    - return the answer\n",
    "    - put the new question & answer in the cache as well\n",
    "- if no, \n",
    "    - reach out to the llm to get the answer. \n",
    "    - Then put the question & answer in the cache in case a similar question will come up again\n",
    "- measure the tokens used \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery\n",
    ")\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "credential = AzureKeyCredential(os.environ[\"AZURE_AI_SEARCH_KEY\"]) if len(os.environ[\"AZURE_AI_SEARCH_KEY\"]) > 0 else DefaultAzureCredential()\n",
    "from pprint import pprint\n",
    "\n",
    "async def ask_question(ask: Ask):\n",
    "    \"\"\"\n",
    "    Ask a question\n",
    "    \"\"\"   \n",
    "    question = ask.question\n",
    "    print(\"=\"*120)\n",
    "    print(\"New Question: \", question)\n",
    "    print(\"=\"*120)\n",
    "    index_name = \"question-semantic-index\"\n",
    "\n",
    "    # create new searchclient using our new index for the questions\n",
    "    search_client = SearchClient(\n",
    "        endpoint=os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"], \n",
    "        index_name=index_name,\n",
    "        credential=credential\n",
    "    )\n",
    "\n",
    "    \n",
    "    # check if the question &  answer is in the cache already\n",
    "    # TODO    \n",
    "    \n",
    "    # create a vectorized query based on the question\n",
    "    vector = VectorizedQuery(vector=get_embedding(question), k_nearest_neighbors=5, fields=\"vector\")\n",
    "\n",
    "    # create search client to retrieve questions from the vector store\n",
    "    found_questions = list(search_client.search(\n",
    "        search_text=None,\n",
    "        query_type=\"semantic\",\n",
    "        semantic_configuration_name=\"question-semantic-config\",\n",
    "        vector_queries=[vector],\n",
    "        select=[\"question\", \"answer\"],\n",
    "        top=5\n",
    "    ))\n",
    "\n",
    "    # filter out questions with low scores\n",
    "    found_questions = [q for q in found_questions if q['@search.score'] > 0.9]\n",
    "    print(\"Found Questions:\")\n",
    "    for found_question in found_questions:\n",
    "        print(\"Question: {}\".format(found_question[\"question\"]))\n",
    "        print(\"Answer: {}\".format(found_question[\"answer\"]))\n",
    "        print(\"----------\")\n",
    "    \n",
    "        \n",
    "\n",
    "    if(found_questions):\n",
    "        print(\"Found a match in the cache.\")\n",
    "        print(\"Cached Question: \", found_questions[0][\"question\"])\n",
    "        print(\"Cached Answer: \", found_questions[0][\"answer\"])\n",
    "        answer=found_questions[0][\"answer\"]\n",
    "    else:\n",
    "        print(\"No match found in the cache.\")\n",
    "        print('Sending a request to LLM')\n",
    "        start_phrase = ask.question\n",
    "        messages=  [{\"role\" : \"assistant\", \"content\" : start_phrase},\n",
    "                     { \"role\" : \"system\", \"content\" : \"Answer this question with a very short answer. Don't answer with a full sentence, and do not format the answer.\"}]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "             model = deployment_name,\n",
    "             messages =messages,\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "        print(\"Answer from LLM: \", answer)\n",
    "\n",
    "    question_document = {\n",
    "        \"question\": question,\n",
    "        \"answer\" : answer,\n",
    "        \"vector\": get_embedding(question),\n",
    "        \"id\": str(search_client.get_document_count()+1)\n",
    "    }\n",
    "    if search_client.upload_documents(question_document):\n",
    "        print(\"Successfully added question to cache.\")\n",
    "    else:\n",
    "        print(\"Failed to add question to cache!\")\n",
    "    \n",
    "\n",
    "    answer_object = Answer(answer=answer)\n",
    "    print(\"Returning answer: \", answer_object)\n",
    "    print(\"=\"*120)\n",
    "    return answer_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this snippet to try your method with several questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "New Question:  Who brings the character of Tony Stark to life in the Marvel Cinematic Universe? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "========================================================================================================================\n",
      "Found Questions:\n",
      "Question: Who brings the character of Tony Stark to life in the Marvel Cinematic Universe? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Who brings the character of Tony Stark to life in the Marvel Cinematic Universe? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: In the Marvel Cinematic Universe, who is the actor that brings Tony Stark to life? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: In the Marvel Cinematic Universe, who is the actor that brings Tony Stark to life? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Which actor plays Tony Stark in the Marvel movies? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Found a match in the cache.\n",
      "Cached Question:  Who brings the character of Tony Stark to life in the Marvel Cinematic Universe? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Cached Answer:  Robert Downey Jr.\n",
      "Successfully added question to cache.\n",
      "Returning answer:  answer='Robert Downey Jr.' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "Answer: answer='Robert Downey Jr.' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "New Question:  In the Marvel Cinematic Universe, who is the actor that brings Tony Stark to life? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "========================================================================================================================\n",
      "Found Questions:\n",
      "Question: In the Marvel Cinematic Universe, who is the actor that brings Tony Stark to life? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: In the Marvel Cinematic Universe, who is the actor that brings Tony Stark to life? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Who brings the character of Tony Stark to life in the Marvel Cinematic Universe? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Who brings the character of Tony Stark to life in the Marvel Cinematic Universe? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Who brings the character of Tony Stark to life in the Marvel Cinematic Universe? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Found a match in the cache.\n",
      "Cached Question:  In the Marvel Cinematic Universe, who is the actor that brings Tony Stark to life? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Cached Answer:  Robert Downey Jr.\n",
      "Successfully added question to cache.\n",
      "Returning answer:  answer='Robert Downey Jr.' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "Answer: answer='Robert Downey Jr.' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "New Question:  Which actor plays Tony Stark in the Marvel movies? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "========================================================================================================================\n",
      "Found Questions:\n",
      "Question: Which actor plays Tony Stark in the Marvel movies? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Which actor plays Tony Stark in the Marvel movies? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Which actor plays Tony Stark in the Marvel movies? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Which actor plays Tony Stark in the Marvel movies? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: In the Marvel Cinematic Universe, who is the actor that brings Tony Stark to life? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Found a match in the cache.\n",
      "Cached Question:  Which actor plays Tony Stark in the Marvel movies? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\n",
      "Cached Answer:  Robert Downey Jr.\n",
      "Successfully added question to cache.\n",
      "Returning answer:  answer='Robert Downey Jr.' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "Answer: answer='Robert Downey Jr.' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "New Question:  What is the capital of France? Paris, London, Rome, Berlin\n",
      "========================================================================================================================\n",
      "Found Questions:\n",
      "Question: What is the capital of France? Paris, London, Rome, Berlin\n",
      "Answer: Paris\n",
      "----------\n",
      "Found a match in the cache.\n",
      "Cached Question:  What is the capital of France? Paris, London, Rome, Berlin\n",
      "Cached Answer:  Paris\n",
      "Successfully added question to cache.\n",
      "Returning answer:  answer='Paris' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "Answer: answer='Paris' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "New Question:  Who is the author of the Harry Potter series? J.K. Rowling, Stephen King, George R.R. Martin, Dan Brown\n",
      "========================================================================================================================\n",
      "Found Questions:\n",
      "Question: Who is the author of the Harry Potter series? J.K. Rowling, Stephen King, George R.R. Martin, Dan Brown\n",
      "Answer: J.K. Rowling\n",
      "----------\n",
      "Question: Who is the author of the Harry Potter series? J.K. Rowling, Stephen King, George R.R. Martin, Dan Brown\n",
      "Answer: J.K. Rowling\n",
      "----------\n",
      "Question: Who is the author of the Harry Potter series? J.K. Rowling, Stephen King, George R.R. Martin, Dan Brown\n",
      "Answer: J.K. Rowling\n",
      "----------\n",
      "Found a match in the cache.\n",
      "Cached Question:  Who is the author of the Harry Potter series? J.K. Rowling, Stephen King, George R.R. Martin, Dan Brown\n",
      "Cached Answer:  J.K. Rowling\n",
      "Successfully added question to cache.\n",
      "Returning answer:  answer='J.K. Rowling' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "Answer: answer='J.K. Rowling' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "New Question:  What is the largest planet in our solar system? Jupiter, Mars, Saturn, Venus\n",
      "========================================================================================================================\n",
      "Found Questions:\n",
      "Question: What is the largest planet in our solar system? Jupiter, Mars, Saturn, Venus\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: What is the largest planet in our solar system? Jupiter, Mars, Saturn, Venus\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: What is the largest planet in our solar system? Jupiter, Mars, Saturn, Venus\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: What is the largest planet in our solar system? Jupiter, Mars, Saturn, Venus\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Found a match in the cache.\n",
      "Cached Question:  What is the largest planet in our solar system? Jupiter, Mars, Saturn, Venus\n",
      "Cached Answer:  Robert Downey Jr.\n",
      "Successfully added question to cache.\n",
      "Returning answer:  answer='Robert Downey Jr.' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "Answer: answer='Robert Downey Jr.' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "New Question:  Which country won the FIFA World Cup in 2018? France, Brazil, Germany, Argentina\n",
      "========================================================================================================================\n",
      "Found Questions:\n",
      "Question: Which country won the FIFA World Cup in 2018? France, Brazil, Germany, Argentina\n",
      "Answer: France\n",
      "----------\n",
      "Question: Which country won the FIFA World Cup in 2018? France, Brazil, Germany, Argentina\n",
      "Answer: France\n",
      "----------\n",
      "Question: Which country won the FIFA World Cup in 2018? France, Brazil, Germany, Argentina\n",
      "Answer: France\n",
      "----------\n",
      "Found a match in the cache.\n",
      "Cached Question:  Which country won the FIFA World Cup in 2018? France, Brazil, Germany, Argentina\n",
      "Cached Answer:  France\n",
      "Successfully added question to cache.\n",
      "Returning answer:  answer='France' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "Answer: answer='France' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "New Question:  Who painted the Mona Lisa? Leonardo da Vinci, Vincent van Gogh, Pablo Picasso, Michelangelo\n",
      "========================================================================================================================\n",
      "Found Questions:\n",
      "Question: Who painted the Mona Lisa? Leonardo da Vinci, Vincent van Gogh, Pablo Picasso, Michelangelo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Who painted the Mona Lisa? Leonardo da Vinci, Vincent van Gogh, Pablo Picasso, Michelangelo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Who painted the Mona Lisa? Leonardo da Vinci, Vincent van Gogh, Pablo Picasso, Michelangelo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Who painted the Mona Lisa? Leonardo da Vinci, Vincent van Gogh, Pablo Picasso, Michelangelo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Question: Who painted the Mona Lisa? Leonardo da Vinci, Vincent van Gogh, Pablo Picasso, Michelangelo\n",
      "Answer: Robert Downey Jr.\n",
      "----------\n",
      "Found a match in the cache.\n",
      "Cached Question:  Who painted the Mona Lisa? Leonardo da Vinci, Vincent van Gogh, Pablo Picasso, Michelangelo\n",
      "Cached Answer:  Robert Downey Jr.\n",
      "Successfully added question to cache.\n",
      "Returning answer:  answer='Robert Downey Jr.' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n",
      "========================================================================================================================\n",
      "Answer: answer='Robert Downey Jr.' correlationToken=None promptTokensUsed=None completionTokensUsed=None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ask = Ask(question=\"Who brings the character of Tony Stark to life in the Marvel Cinematic Universe? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)\n",
    "\n",
    "ask = Ask(question=\"In the Marvel Cinematic Universe, who is the actor that brings Tony Stark to life? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)\n",
    "\n",
    "ask = Ask(question=\"Which actor plays Tony Stark in the Marvel movies? Robert Downey Jr., Chris Hemsworth, Chris Evans, Mark Ruffalo\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)\n",
    "\n",
    "ask = Ask(question=\"What is the capital of France? Paris, London, Rome, Berlin\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)\n",
    "\n",
    "ask = Ask(question=\"What is the capital of France? London, Paris, Berlin, Rome\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)\n",
    "\n",
    "ask = Ask(question=\"Who is the author of the Harry Potter series? J.K. Rowling, Stephen King, George R.R. Martin, Dan Brown\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)\n",
    "\n",
    "ask = Ask(question=\"What is the largest planet in our solar system? Jupiter, Mars, Saturn, Venus\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)\n",
    "\n",
    "ask = Ask(question=\"Which country won the FIFA World Cup in 2018? France, Brazil, Germany, Argentina\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)\n",
    "\n",
    "ask = Ask(question=\"Who painted the Mona Lisa? Leonardo da Vinci, Vincent van Gogh, Pablo Picasso, Michelangelo\", type=QuestionType.multiple_choice)\n",
    "answer = await ask_question(ask)\n",
    "print('Answer:', answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you transfer your code changes into main.py (or additional files). Then redeploy your container using this command.\n",
    "```\n",
    "bash ./azd-hooks/deploy.sh phase4 $AZURE_ENV_NAME\n",
    "```\n",
    "Make sure to provide the URL of your endpoint in the team portal!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
